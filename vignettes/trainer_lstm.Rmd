---
title: "Comparing Different Training Algorithms"
author: "Andreas M. Brandmaier"
date: "12/12/2019"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache=FALSE)
```

In this document, we will compare the convergence of different training algorithms for a given dataset and network architecture. The data will be simulated from a simple toy problem, in which there are four i.i.d. Gaussian predictors and a single metric outcome. 

## Load Library

First, we load the shared library.

```{r pressure, echo=TRUE}


  dyn.load(paste("../bnnlib", .Platform$dynlib.ext, sep=""))
  source("../bnnlib.R")
  cacheMetaData(1)

```

## Generate Data

Next, we simulate some data in a data.frame and convert this to bnn's Sequence format:

```{r}
set.seed(34235)

seq <- SequenceSet()
test <- SequenceSet()
delay <- 20

num.seq <- 10

for (i in 1:num.seq) {
len <- 50
# create a sequence from scratch
input <- rep(0,len)
output <- rep(0,len)

pos <- sample(1:(len-delay-1),1)
cat(i,".: Position=",pos,"\n")
input[pos]<-1
output[pos+delay]<-1

seq1<-Sequence(input,output,len)

if (i > num.seq/2) {
  SequenceSet_add_copy_of_sequence(test, seq1)
} else {
  SequenceSet_add_copy_of_sequence(seq,seq1)
}
  
}


```

This is a sanity check, whether the input and output size is correct.

```{r}
Sequence_get_target_size(seq1)
Sequence_get_input_size(seq1)
SequenceSet_get_input_size(seq)
SequenceSet_get_target_size(seq)
```

## Create Network

Now, we create a feed-forward network with a single hidden layer of 10 neurons. 

```{r}
net <- LSTMNetwork(1,4,1)

```

## Trainer

Let's generate a list of different training algorithms and run them each for 100 steps. Save the results in `err.data`:

```{r}

trainer <- list(
                 BackpropTrainer(net), BackpropTrainer(net),
                 ARPropTrainer(net),
                  ImprovedRPropTrainer(net), ImprovedRPropTrainer(net))

steps <- 80
steps.per.iteration <- 1
err.data <- matrix(data=NA,nrow=length(trainer)*steps,ncol=3)
err.data <- data.frame(err.data)
names(err.data) <- c("error","trainer","step")
k <- 1

for (i in 1:length(trainer)) {
  bp <- trainer[[i]]
  Network_reinitialise(net)

  for (j in 1:steps) {
    
    #Trainer_train__SWIG_0(bp, seq, steps.per.iteration)
    #Trainer_train2(bp, seq, 1)
    Trainer_train_sequenceset(bp, seq)
    
    err.data[k,1] <- Network_evaluate_training_error__SWIG_0(net, seq)
    err.data[k,2] <- paste0(i,":", Trainer_get_name(bp))
    err.data[k,3] <- j
    k<-k+1
  }
}

err.data$trainer<-factor(err.data$trainer)
```

Plot the errors

```{r}
library(ggplot2)

ggplot(data=err.data, 
       aes(x=step,y=error,group=trainer,col=trainer))+
  geom_point()+
  geom_line()+
  theme_minimal()
```


# Test set predictions

```{r}
#s1 = SequenceSet_get(test, 0)
s1 = SequenceSet_get(seq, 0)
Network_activate(net, s1)

ln <- Sequence_size(s1)
  
 xs <- rep(NA, ln)
 for (i in 1:ln) {
		x <- Network_get_output(net,i)
		x <- .Call('R_swig_toValue', x, package="bnnlib") 
		xs[i] <- x
 }

 
ggplot(data=data.frame(x=1:ln, y=x), aes(x=x,y=y))+geom_line() 
```